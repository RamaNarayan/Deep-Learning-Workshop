{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "This Jupyter notebook was created for the 2018 AIS Deep Learning for Computer Vision Workshop for the 2018 AI Conference at UT Dallas and focuses on using deep learning models such as convolutional neural networks to perform interesting computer vision tasks such as style transfer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "Introduction on concepts,\n",
    "Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding. - Wikipedia\n",
    "\n",
    "Computer vision concentrates on mimicking human vision right from perception to decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "Deep learning is a subset of machine learning. The word 'Deep' refers to the multiple layers that the data is getting transformed. Usually there'll be more than one hidden layer between the input and output layer. There are multiple neural network architectures for deep learning but today we'll just look into convoluted neural networks.\n",
    "\n",
    "<img src=\"images/DeepLearning.png\" style=\"width:500px;height:150px;\">\n",
    "\n",
    "<p style=\"font-size:6pt\">Image src - http://www.global-engage.com/life-science/deep-learning-in-digital-pathology/ </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convoluted neural networks\n",
    "\n",
    "Consider a network that has a naive approach of matching pixel to a pixel. Will it work if the same image is truncated, deformed or rotated? CNN does not match pixel to pixel. Compares piece by piece which we call features/filter. Feature match is done by cross correlation(not convolution!). Requires little preprocessing than the other image classification algorithms. This is because the network learns the filters by itself.\n",
    "\n",
    "<img src=\"images/MLvsDL.png\" style=\"width:500px;height:200px;\">\n",
    "\n",
    "<p style=\"font-size:6pt\">Image src - https://semiengineering.com/deep-learning-spreads/ </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "A 4D Array. [batchSize][height][width][depth]\n",
    "Depth is usually 3 for color image (RGB) and 1 for gray scale image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "Features/Filter - 2D array of values. How do we know where that there is a feature match in the new image? Simple - We try it out in all possible locations.\n",
    "\n",
    "filter - [filter_height, filter_width, in_channels, out_channels] . in_channels - depth of the filter and out_channels - no of filters.\n",
    "\n",
    "eg: filter = [2, 2, 1, 64]\n",
    "\n",
    "<img src=\"images/filter.PNG\" style=\"width:200px;height:100px;\">\n",
    "\n",
    "So where does this feature matching and feature extraction happens? - Convolutional layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layer\n",
    "Place where humongous computation takes place. Correlation of a feature with all possible positions in an image. One image is converted into stack of images.At the end, tensor will have [image][height][width][filteredImagesActivationMaps]\n",
    "\n",
    "X = [1,224,224,3]\n",
    "\n",
    "filter = [2, 2, 1, 64]\n",
    "\n",
    "tf.nn.conv2d(X, filter, strides =[1,2,2,1], padding='VALID'). \n",
    "\n",
    "Reference - https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\n",
    "\n",
    "Resulting tensor = [1,224,224,64]\n",
    "\n",
    "<img src=\"images/crossCorrelation.PNG\" style=\"width:400px;height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "Determines when the neuron should fire. Common activation function includes sigmoid, tanh and Relu. Rectified Linear Units - a non linear activation function.\n",
    "\n",
    "$$f(x) = max(0,x)$$\n",
    "\n",
    "eg: tf.nn.relu(conv2d_layer)\n",
    "\n",
    "Resulting tensor has non negative values for its features.\n",
    "\n",
    "Reference - https://www.tensorflow.org/api_docs/python/tf/nn/relu\n",
    "\n",
    "<img src=\"images/relu.PNG\" style=\"width:150px;height:150px;\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling\n",
    "Shrink each mini-image in the image stack. Idea is that exact location of the feature in the neighborhood is not required.This is how we capture even when particular feature is rotated or tilted. All we care about is that if it exists. Reduces computation time and controls overfitting. There are various pooling methods available - Max Pooling, L2-norm pooling and Average pooling. Max pooling is selecting the max value in the neighbourhood. Average pooling is the average of the values in the neighbourhood.\n",
    "\n",
    "<I>eg : tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')</I>\n",
    "\n",
    "Reference - https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool\n",
    "\n",
    "<img src=\"images/pooling_desc.PNG\" style=\"width:500px;height:150px;\">\n",
    "\n",
    "<img src=\"images/averagePoolReduction.PNG\" style=\"width:500px;height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Layer\n",
    "Each feature becomes a list of vote values.The tensor is flattened.During training, higher weights will have a stronger vote.When new image is presented, whichever weight had a higher vote - those values are taken and averaged. \n",
    "\n",
    "eg: [1,2,2,64] becomes [1,256].\n",
    "\n",
    "How do you know the voting weights and the features in convolutional layer? - Backpropagation and optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation and Optimization\n",
    "Error - Expected - Actual. This error function is called as the loss function. Aim is to minimize the loss function. Performs gradient descent based on the error. Once a minima is reached, the values for feature and voting weights are settled.\n",
    "\n",
    "The optimization algorithm repeats a two phase cycle, propagation and weight update. When an input vector is presented to the network, it is propagated forward through the network, layer by layer, until it reaches the output layer. The output of the network is then compared to the desired output, using a loss function. The resulting error value is calculated for each of the neurons in the output layer. The error values are then propagated from the output back through the network, until each neuron has an associated error value that reflects its contribution to the original output.\n",
    "\n",
    "Backpropagation uses these error values to calculate the gradient of the loss function. In the second phase, this gradient is fed to the optimization method, which in turn uses it to update the weights, in an attempt to minimize the loss function.\n",
    "\n",
    "There are many variations of gradient descent. We won't be covering them. We'll be using Adam optimizer. Research paper - https://arxiv.org/abs/1412.6980. Faster training time than using stochastic gradient descent.\n",
    "\n",
    "<img src=\"images/gradientDescent.png\" style=\"width:500px;height:150px;\">\n",
    "\n",
    "<p style=\"font-size:6pt\">Image src - https://hackernoon.com/gradient-descent-aynk-7cbe95a778da?gi=118fb21b98f8 </p>\n",
    "\n",
    "One important thing to choose for optimization is the learning rate. Learning rate is the rate at which the weights are pushed towards the gradient. Lower learning rate increases training time and higher learning rate may not converge at all.\n",
    "\n",
    "\n",
    "<img src=\"images/learning_rate.jpg\" style=\"width:500px;height:150px;\">\n",
    "\n",
    "<p style=\"font-size:6pt\">Image src - https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0 </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning and VGG Model\n",
    "\n",
    "We'll be using a pre-trained network. Transfer learning is using a pretrained network and modifying its top layers to suit our classification problem. Why it works? - The lower layers of pre-trained network recognizes low level features such as edges. We'll be using VGG model. Research paper - https://arxiv.org/abs/1409.1556\n",
    "\n",
    "<img src=\"images/VGG_model.png\" style=\"width:800px;height:800px;\">\n",
    "\n",
    "<img src=\"images/vgg16.png\">\n",
    "\n",
    "<p style=\"font-size:6pt\">Image src - https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/ </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "Converts to a probability distribution. Induces some fuzzy thinking into the network. Consider a network for selecting between k categories. It is designed such that at the last step there are k computed real values $V_{1}$,..,$V_{k}$. The SoftMax converts these values into probabilities as follows:\n",
    "\n",
    "Forward propagation:    \n",
    "\n",
    "$$q_{i} = e^{V_{i}} ; i = 1,..,k $$ , \n",
    "\n",
    "\n",
    "$$Z = SUM(q_{i}) ; i = 1,..k $$ ,\n",
    "\n",
    "\n",
    "$$p_{i}=\\frac{q_{i}}{Z} ; i = 1,..,k $$\n",
    "\n",
    "\n",
    "Back propagation:\n",
    "\n",
    "$$E = E_{j} = -ln (p_{j}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "This algorithm was created by Gatys et al. (2015) (https://arxiv.org/abs/1508.06576) The research paper showed how we can separate style and content from an image. In this workshop, we are going to seperate the content from one image, style from a different image and merge them together to produce art! We'll call them content image and style image respectively.\n",
    "\n",
    "<img src=\"images/UTD_generated.png\" style=\"width:1000px;height:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "Let's go ahead and import tensorflow as well as some other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import imageio\n",
    "from matplotlib.pyplot import imshow\n",
    "from nst_utils import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by creating an interactive session in tensorflow. Session is an enviroment in which your operations run. Its basically an encapsulation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start interactive session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Content Image\n",
    "Lets load the content image, reshape and normalize it. Our content image is an aerial vew of UTD campus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = imageio.imread(\"images/UTDAerial.jpg\")\n",
    "imshow(content_image)\n",
    "\n",
    "# Reshape image to mach expected input of VGG\n",
    "print(content_image.shape)\n",
    "content_image = np.reshape(content_image, ((1,) + content_image.shape))\n",
    "print(content_image.shape)\n",
    "\n",
    "# Substract the mean to match the expected input of VGG\n",
    "MEANS = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "content_image = content_image - MEANS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Style Image\n",
    "Lets load the Style image, reshape and normalize it. Our Style image is Van Gogh's \"Starry Night\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = imageio.imread(\"images/starryNight.jpg\")\n",
    "imshow(style_image)\n",
    "\n",
    "# Reshape image to mach expected input of VGG\n",
    "print(style_image.shape)\n",
    "style_image = np.reshape(style_image, ((1,) + style_image.shape))\n",
    "print(style_image.shape)\n",
    "\n",
    "# Substract the mean to match the expected input of VGG\n",
    "MEANS = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "style_image = style_image - MEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pre-Trained Model\n",
    "Lets load the pre-trained model. We'll be using VGG-19 model. This is indeed a deep CNN. You can download the model from here http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_vgg_model(\"pretrained-model/imagenet-vgg-verydeep-19.mat\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = generate_noise_image(content_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function for backtracking\n",
    "We will calculate two costs.\n",
    "\n",
    "1. Content Cost - cc\n",
    "2. Style Cost - sc\n",
    "3. Total Cost - tc = α*cc + β*sc\n",
    "\n",
    "Aim is to minimize the total cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content cost\n",
    "Cost that determines how close the generated image is to the content image. We use normalized version of squared euclidean distance to find it. We will have to chose some layer's activations to represent the content of an image. In practice, you'll get the most visually pleasing results if you choose a layer in the middle of the network--neither too shallow nor too deep. Feel free to experiment!\n",
    "\n",
    "Now, set the image C as the input to the pretrained VGG network, and run forward propagation. Let $a^{(C)}$ be the hidden layer activations in the layer you had chosen. This will be a $n_H \\times n_W \\times n_C$ tensor. Repeat this process with the image G: Set G as the input, and run forward progation. \n",
    "Let $$a^{(G)}$$ be the corresponding hidden layer activation. We will define as the content cost function as:\n",
    "\n",
    "$$cc(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2\\tag{1} $$\n",
    "\n",
    "Here, $n_H, n_W$ and $n_C$ are the height, width and number of channels of the hidden layer you have chosen, and appear in a normalization term in the cost. For clarity, note that $a^{(C)}$ and $a^{(G)}$ are the volumes corresponding to a hidden layer's activations.\n",
    "\n",
    "<img src=\"images/contentEdges.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_content_cost\n",
    "\n",
    "def compute_content_cost(a_C, a_G):\n",
    "    \"\"\"\n",
    "    Computes the content cost\n",
    "    \n",
    "    Arguments:\n",
    "    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C \n",
    "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G\n",
    "    \n",
    "    Returns: \n",
    "    J_content -- scalar that you compute using equation 1 above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from a_G (≈1 line)\n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    # Reshape a_C and a_G (≈2 lines)\n",
    "    a_C_unrolled = tf.transpose(a_C)\n",
    "    a_G_unrolled = tf.transpose(a_G)\n",
    "    \n",
    "    # compute the cost with tensorflow (≈1 line)\n",
    "    J_content = (1/ (4* n_H * n_W * n_C)) * tf.reduce_sum(tf.pow((a_G_unrolled - a_C_unrolled), 2))\n",
    "    \n",
    "    return J_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Cost\n",
    "\n",
    "Cost that determines how close the style of generated image is to the style of the style image. How do you measure style of an image? For style, we should ignore the spatial information. The research showed Gram matrix gives pretty much good info about the style. In linear algebra, the Gram matrix G of a set of vectors $(v_{1},\\dots ,v_{n})$ is the matrix of dot products, whose entries are ${\\displaystyle G_{ij} = v_{i}^T v_{j} = np.dot(v_{i}, v_{j})  }$. In other words, $G_{ij}$ compares how similar $v_i$ is to $v_j$: If they are highly similar, you would expect them to have a large dot product, and thus for $G_{ij}$ to be large.\n",
    "\n",
    "The result is a matrix of dimension $(n_C,n_C)$ where $n_C$ is the number of filters. The value $G_{ij}$ measures how similar the activations of filter $i$ are to the activations of filter $j$.\n",
    "\n",
    "By capturing the prevalence of different types of features ($G_{ii}$), as well as how much different features occur together ($G_{ij}$), the Style matrix $G$ measures the style of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gram_matrix\n",
    "\n",
    "def gram_matrix(A):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    A -- matrix of shape (n_C, n_H*n_W)\n",
    "    \n",
    "    Returns:\n",
    "    GA -- Gram matrix of A, of shape (n_C, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    GA = tf.matmul(A, tf.transpose(A))\n",
    "    \n",
    "    return GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the style cost now. We follow the same procedure as content cost. We choose an activation layer, find the gram matrix of both style and generated image and calculate squared euclidean distance.\n",
    "\n",
    "$$sc(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2\\tag{2} $$\n",
    "\n",
    "where $G^{(S)}$ and $G^{(G)}$ are respectively the Gram matrices of the \"style\" image and the \"generated\" image, computed using the hidden layer activations for a particular hidden layer in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_layer_style_cost\n",
    "\n",
    "def compute_layer_style_cost(a_S, a_G):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S \n",
    "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G\n",
    "    \n",
    "    Returns: \n",
    "    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve dimensions from a_G (≈1 line)\n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    # Reshape the images to have them of shape (n_H*n_W, n_C) (≈2 lines)\n",
    "    a_S = tf.transpose(tf.reshape(a_S, [n_H*n_W, n_C]))\n",
    "    a_G = tf.transpose(tf.reshape(a_G, [n_H*n_W, n_C]))\n",
    "\n",
    "    # Computing gram_matrices for both images S and G (≈2 lines)\n",
    "    GS = gram_matrix(a_S)\n",
    "    GG = gram_matrix(a_G)\n",
    "\n",
    "    # Computing the loss (≈1 line)\n",
    "    J_style_layer = (1./(4 * n_C**2 * (n_H*n_W)**2)) * tf.reduce_sum(tf.pow((GS - GG), 2))\n",
    "\n",
    "    \n",
    "    return J_style_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get better results if we \"merge\" style costs from several different layers. You can combine the style costs for different layers as follows:\n",
    "\n",
    "$$sc(S,G) = \\sum_{l} \\lambda^{[l]} sc^{[l]}(S,G)$$\n",
    "\n",
    "where the values for $\\lambda^{[l]}$ are given in `STYLE_LAYERS`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_LAYERS = [\n",
    "    ('conv1_1', 0.2),\n",
    "    ('conv2_1', 0.2),\n",
    "    ('conv3_1', 0.2),\n",
    "    ('conv4_1', 0.2),\n",
    "    ('conv5_1', 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_style_cost(model, STYLE_LAYERS):\n",
    "    \"\"\"\n",
    "    Computes the overall style cost from several chosen layers\n",
    "    \n",
    "    Arguments:\n",
    "    model -- our tensorflow model\n",
    "    STYLE_LAYERS -- A python list containing:\n",
    "                        - the names of the layers we would like to extract style from\n",
    "                        - a coefficient for each of them\n",
    "    \n",
    "    Returns: \n",
    "    J_style -- tensor representing a scalar value, style cost defined above by equation (2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the overall style cost\n",
    "    J_style = 0\n",
    "\n",
    "    for layer_name, coeff in STYLE_LAYERS:\n",
    "\n",
    "        # Select the output tensor of the currently selected layer\n",
    "        out = model[layer_name]\n",
    "\n",
    "        # Set a_S to be the hidden layer activation from the layer we have selected, by running the session on out\n",
    "        a_S = sess.run(out)\n",
    "\n",
    "        # Set a_G to be the hidden layer activation from same layer. Here, a_G references model[layer_name] \n",
    "        # and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that\n",
    "        # when we run the session, this will be the activations drawn from the appropriate layer, with G as input.\n",
    "        a_G = out\n",
    "        \n",
    "        # Compute style_cost for the current layer\n",
    "        J_style_layer = compute_layer_style_cost(a_S, a_G)\n",
    "\n",
    "        # Add coeff * J_style_layer of this layer to overall style cost\n",
    "        J_style += coeff * J_style_layer\n",
    "\n",
    "    return J_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Cost \n",
    "Finally, let's create a cost function that minimizes both the style and the content cost. The formula is: \n",
    "\n",
    "$$J(G) = \\alpha cc(C,G) + \\beta sc(S,G)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: total_cost\n",
    "\n",
    "def total_cost(J_content, J_style, alpha = 10, beta = 40):\n",
    "    \"\"\"\n",
    "    Computes the total cost function\n",
    "    \n",
    "    Arguments:\n",
    "    J_content -- content cost coded above\n",
    "    J_style -- style cost coded above\n",
    "    alpha -- hyperparameter weighting the importance of the content cost\n",
    "    beta -- hyperparameter weighting the importance of the style cost\n",
    "    \n",
    "    Returns:\n",
    "    J -- total cost as defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    J = alpha * J_content + beta * J_style\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get to the actual execution of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the content image to be the input of the VGG model.  \n",
    "sess.run(model['input'].assign(content_image))\n",
    "\n",
    "# Select the output tensor of layer conv4_2\n",
    "out = model['conv4_2']\n",
    "\n",
    "# Set a_C to be the hidden layer activation from the layer we have selected\n",
    "a_C = sess.run(out)\n",
    "\n",
    "# Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2'] \n",
    "# and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that\n",
    "# when we run the session, this will be the activations drawn from the appropriate layer, with G as input.\n",
    "a_G = out\n",
    "\n",
    "# Compute the content cost\n",
    "J_content = compute_content_cost(a_C, a_G)\n",
    "\n",
    "# Assign the input of the model to be the \"style\" image \n",
    "sess.run(model['input'].assign(style_image))\n",
    "\n",
    "# Compute the style cost\n",
    "J_style = compute_style_cost(model, STYLE_LAYERS)\n",
    "\n",
    "J = total_cost(J_content, J_style, alpha = 10, beta = 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer (1 line)\n",
    "optimizer = tf.train.AdamOptimizer(2.0)\n",
    "\n",
    "# define train_step (1 line)\n",
    "train_step = optimizer.minimize(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_nn(sess, input_image, num_iterations = 300):\n",
    "    \n",
    "    # Initialize global variables (you need to run the session on the initializer)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "       \n",
    "    # Run the noisy input image (initial generated image) through the model. Use assign().\n",
    "\n",
    "    sess.run(model['input'].assign(input_image))\n",
    "\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "    \n",
    "        # Run the session on the train_step to minimize the total cost\n",
    "\n",
    "        sess.run(train_step)\n",
    "\n",
    "        \n",
    "        # Compute the generated image by running the session on the current model['input']\n",
    "\n",
    "        generated_image = sess.run(model['input'])\n",
    "\n",
    "\n",
    "        # Print every 20 iteration.\n",
    "        if i%20 == 0:\n",
    "            Jt, Jc, Js = sess.run([J, J_content, J_style])\n",
    "            print(\"Iteration \" + str(i) + \" :\")\n",
    "            print(\"total cost = \" + str(Jt))\n",
    "            print(\"content cost = \" + str(Jc))\n",
    "            print(\"style cost = \" + str(Js))\n",
    "            \n",
    "            # save current generated image in the \"/output\" directory\n",
    "            save_image(\"output/\" + str(i) + \".png\", generated_image)\n",
    "    \n",
    "    # save last generated image\n",
    "    save_image('output/generated_image.jpg', generated_image)\n",
    "    \n",
    "    return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn(sess, generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "Coursera - https://github.com/enggen/Deep-Learning-Coursera"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
